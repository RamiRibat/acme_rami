# Copyright 2018 DeepMind Technologies Limited. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""SAC Builder."""
from typing import Iterator, List, Optional

import acme

from acme import adders
from acme import core
from acme import specs

from acme.adders import reverb as adders_reverb

from acme.agents.jax import actor_core as actor_core_lib
from acme.agents.jax import actors
from acme.agents.jax import builders
from acme.agents.jax import normalization
from acme.agents.jax.sac import config as sac_config
from acme.agents.jax.sac import learning
from acme.agents.jax.sac import networks as sac_networks

from acme.datasets import reverb as datasets

from acme.jax import networks as networks_lib
from acme.jax import utils
from acme.jax import variable_utils

from acme.utils import counting
from acme.utils import loggers

import jax
import optax
import reverb
from reverb import rate_limiters

from acme.agents.jax.sac.config import target_entropy_from_env_spec


@normalization.input_normalization_builder
class SACBuilder(builders.ActorLearnerBuilder[sac_networks.SACNetworks,
                                              actor_core_lib.FeedForwardPolicy,
                                              reverb.ReplaySample]):
	"""SAC Builder."""

	def __init__(
		self,
		config: sac_config.SACConfig,
	):
		"""Creates a SAC learner, a behavior policy and an eval actor.

		Args:
			config: a config with SAC hps
		"""
		self._config = config


	def make_replay_tables(
		self,
		environment_spec: specs.EnvironmentSpec,
		policy: actor_core_lib.FeedForwardPolicy,
	) -> List[reverb.Table]:
		"""Create tables to insert data into."""

		del policy

		# dummy_actor_state = policy.init(jax.random.PRNGKey(0))
		# extras_spec = policy.get_extras(dummy_actor_state)
		# step_spec = adders_reverb.create_step_spec(
		# 	environment_spec=environment_spec,
		# 	extras_spec=extras_spec
		# )

		# Create the rate limiter.
		if self._config.samples_per_insert:
			samples_per_insert_tolerance = (
			self._config.samples_per_insert_tolerance_rate *
			self._config.samples_per_insert)
			error_buffer = self._config.min_replay_size * samples_per_insert_tolerance
			limiter = rate_limiters.SampleToInsertRatio(
			min_size_to_sample=self._config.min_replay_size, # = 10k
			samples_per_insert=self._config.samples_per_insert, # = 256
			error_buffer=error_buffer # 10k x 0.1 = 1k
			)
		else:
			limiter = rate_limiters.MinSize(self._config.min_replay_size)

		signature = adders_reverb.NStepTransitionAdder.signature(environment_spec)
		# signature = sw.infer_signature(
		# 	configs=_make_adder_config(
		# 		step_spec,
		# 		self._config.n_step,
		# 		self._config.replay_table_name
		# 	),
		# 	step_spec=step_spec,
		# )

		return [
			reverb.Table( # transition-based
				name=self._config.replay_table_name,
				sampler=reverb.selectors.Uniform(),
				remover=reverb.selectors.Fifo(),
				max_size=self._config.max_replay_size,
				rate_limiter=limiter, # What?
				signature=signature # What?
			)
		]


	def make_dataset_iterator(
		self,
		replay_client: reverb.Client
	) -> Iterator[reverb.ReplaySample]:
		"""Create a (dataset iterator) to use for learning/updating the agent."""

		dataset = datasets.make_reverb_dataset(
			table=self._config.replay_table_name,
			server_address=replay_client.server_address,
			batch_size=(self._config.batch_size * self._config.num_sgd_steps_per_step),
			prefetch_size=self._config.prefetch_size
		)

		dataset = dataset.as_numpy_iterator()

		return utils.device_put(
			iterable=dataset,
			device=jax.devices()[0]
		)


	def make_adder(
		self, replay_client: reverb.Client,
		environment_spec: Optional[specs.EnvironmentSpec],
		policy: Optional[actor_core_lib.FeedForwardPolicy]
	) -> Optional[adders.Adder]:
		"""Create an adder which (records data) generated by the actor/environment."""
		
		del environment_spec, policy # DEPRECATED?

		return adders_reverb.NStepTransitionAdder(
			client=replay_client,
			priority_fns={self._config.replay_table_name: None},
			n_step=self._config.n_step,
			discount=self._config.discount
		)


	def make_learner(
		self,
		random_key: networks_lib.PRNGKey,
		networks: sac_networks.SACNetworks,
		dataset: Iterator[reverb.ReplaySample],
		logger_fn: loggers.LoggerFactory,
		environment_spec: specs.EnvironmentSpec,
		replay_client: Optional[reverb.Client] = None,
		counter: Optional[counting.Counter] = None,
	) -> core.Learner:
		target_entropy = target_entropy_from_env_spec(environment_spec) # Rami: removed from run_sac.py
		del environment_spec, replay_client

		# Create optimizers
		policy_optimizer = optax.adam(learning_rate=self._config.learning_rate)
		q_optimizer = optax.adam(learning_rate=self._config.learning_rate)

		# if self._config.clipping:
		# 	policy_optimizer = optax.chain(
		# 		optax.clip_by_global_norm(40.), policy_optimizer)
		# 	q_optimizer = optax.chain(
		# 		optax.clip_by_global_norm(40.), q_optimizer)

		return learning.SACLearner(
			rng=random_key,
			networks=networks,
			policy_optimizer=policy_optimizer,
			q_optimizer=q_optimizer,
			tau=self._config.tau,
			discount=self._config.discount,
			entropy_coefficient=self._config.entropy_coefficient,
			# target_entropy=self._config.target_entropy,
			target_entropy=target_entropy,
			reward_scale=self._config.reward_scale,
			num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,
			reset_interval=self._config.reset_interval,
			iterator=dataset,
			counter=counter,
			logger=logger_fn('learner'),
		)


	def make_policy( # used indirectly by jax.experiment.run_exp through jax.experiment.config
		self,
		networks: sac_networks.SACNetworks,
		environment_spec: specs.EnvironmentSpec,
		evaluation: bool = False
	) -> actor_core_lib.FeedForwardPolicy:
		"""Construct the policy."""
		del environment_spec

		return sac_networks.apply_policy_and_sample(networks, eval_mode=evaluation)


	def make_actor( # for training and evaluation
		self,
		random_key: networks_lib.PRNGKey,
		policy: actor_core_lib.FeedForwardPolicy,
		environment_spec: specs.EnvironmentSpec,
		variable_source: Optional[core.VariableSource] = None, # learner
		adder: Optional[adders.Adder] = None,
	) -> acme.Actor:
		del environment_spec

		assert variable_source is not None

		actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)

		# Inference happens on CPU, so it's better to move variables there too.
		variable_client = variable_utils.VariableClient(
			variable_source, 'policy', device='cpu'
		)

		actor = actors.GenericActor(
			actor_core, random_key, variable_client, adder, backend='cpu'
		)

		return actor
