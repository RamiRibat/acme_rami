#!/bin/bash

#SBATCH --job-name=test_job
#SBATCH --output=/h/rahmed/logdir/slurm/output-%N-%j.out          # LINES 4, 5 - NOTE: for output and error, use absolute paths that exists already
#SBATCH --error=/h/rahmed/logdir/slurm/error-%N-%j.err            # LINES 4, 5 - NOTE: %N and %j will be replaced by the host name and job id, respectively
#SBATCH --open-mode=append
#SBATCH --partition=gpu                                           # self-explanatory, set to your preference (e.g. gpu or cpu on MaRS, p100, t4, or cpu on Vaughan)
#SBATCH --cpus-per-task=1                                         # self-explanatory, set to your preference
#SBATCH --ntasks-per-node=1
#SBATCH --mem=4G                                                  # self-explanatory, set to your preference
#SBATCH --gres=gpu:1                                              # NOTE: you need a GPU for CUDA support; self-explanatory, set to your preference 
#SBATCH --nodes=1
#SBATCH --qos=normal                                              # for "high" and "deadline" QoS, refer to https://support.vectorinstitute.ai/AboutVaughan2


echo "..Initialize.."

# put your command here

# bash script.sh

# TASK_SUITES=(
#     # 'gym'
#     'control'
#     # 'atari26'
#     # 'atari52'
#     # 'atari57'
# )


# DATETIME=$(date +'%Y%m%d:%H%M%S')

# ID="vector_"$DATETIME

# SEEDS=(1)

# GPUS=(0)

# source ~/.bashrc

# conda activate acme

# for SUITE in ${TASK_SUITES[*]}; do
#     for s in ${!SEEDS[*]}; do
#         CUDA_VISIBLE_DEVICES=${GPUS[s]} python run_d4pg.py --acme_id $ID --suite $SUITE --seed ${SEEDS[s]} &
#     done
# done

# conda deactivate
